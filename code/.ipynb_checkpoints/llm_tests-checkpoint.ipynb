{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2450dd-913e-4b90-a458-3e4d53d4be18",
   "metadata": {},
   "source": [
    "# Large language model application for hierarchical summarization of scientific papers\n",
    "### Fedor Sobolevsky, MIPT 2025\n",
    "\n",
    "This is a notebook containing tests for my bachelor thesis *«Application of Large Language Models for Hierarchical Summarization of Scientific Papers»*. Here I use the text tree edit distance (TTED) metric introduced in this work to assess large language models (LLMs) in application to hierarchical summarization of texts in the form of sentence-based mind maps. The domain of texts I chose to investigate is the scientific domain, but the methods described below are designed to be applicable to any type of texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8490ae5-221d-48df-ac9c-a78dae4feeab",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bf6bf39-0cfc-46c2-b39c-0d654964bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langmem import create_prompt_optimizer\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "\n",
    "from tted.computation import text_tree_distance\n",
    "from tted.tree_format import Node, json_to_node, dict_to_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2bf588-7ad7-4757-9d1a-94b013b881af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(filename: str):\n",
    "    '''\n",
    "    Returns text from text file.\n",
    "    '''\n",
    "    with open(filename, \"r\") as file:\n",
    "        content = file.read()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf2d4cc-1c71-444b-b22d-5b1534452c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(filename: str):\n",
    "    '''\n",
    "    Function for pdf document parsing. Returns the pdf text.\n",
    "    '''\n",
    "    loader = PyPDFLoader(filename)\n",
    "    pages = loader.load()\n",
    "    text = \" \".join([p.page_content for p in pages])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47050e03-0e06-4e5e-a133-5cc86fae53e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_text(text: str):\n",
    "    '''\n",
    "    Extract JSON data from model response containing a JSON text tree.\n",
    "    \n",
    "    Returns the text tree in JSON format or throws an error if the response format is incorrect.\n",
    "    '''\n",
    "    try:\n",
    "        json_tree = json.loads(text[text.find('{'): text.rfind('}')+1])\n",
    "        return json_tree\n",
    "    except Exception as e:\n",
    "        print('Cannot extract JSON from text:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d7a44-dec2-4d45-873f-45831ab38805",
   "metadata": {},
   "source": [
    "### Method 1: Direct Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68bd3e47-af38-46ce-ad71-b01c8b35216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(prompt_file: str, document_pdf: str):\n",
    "    '''\n",
    "    Create prompt for LLM from prompt template and article PDF.\n",
    "\n",
    "    Arguments:\n",
    "        prompt_file: str - name of text file containing prompt template.\n",
    "        document_pdf: str - name of PDF file containing article.\n",
    "    '''\n",
    "    prompt = read_text(prompt_file) + parse_pdf(document_pdf)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def prompt_model(prompt: str, model):\n",
    "    '''\n",
    "    Prompts model with given text prompt.\n",
    "    '''\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82619d79-6639-4fa9-b010-38ca85ab481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Mistral API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import langchain_mistralai\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "if \"MISTRAL_API_KEY\" not in os.environ:\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter your Mistral API key: \")\n",
    "\n",
    "model = ChatMistralAI(model='mistral-large-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6521cc-e89a-4325-83a1-f9c770e7077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens used: 16279\n"
     ]
    }
   ],
   "source": [
    "prompt = create_prompt(prompt_file='prompts/human_prompt_0.txt', document_pdf='data/papers/paper_0.pdf')\n",
    "model_response = prompt_model(prompt=prompt, model=model)\n",
    "\n",
    "print('Tokens used:', model_response.usage_metadata['total_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd31d84-bbd3-45cc-8645-5cbe9d395eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Summarization: Scaling Up Multi-Document Summarization.\n",
      "-Multi-document summarization (MDS) systems have been designed for short, unstructured summaries of 10-15 documents, and are inadequate for larger document collections.\n",
      "--We propose a new approach to scaling up summarization called hierarchical summarization, and present the first implemented system, SUMMA.\n",
      "---SUMMA produces a hierarchy of relatively short summaries, in which the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest.\n",
      "---SUMMA optimizes for coherence as well as coverage of salient information.\n",
      "---In an Amazon Mechanical Turk evaluation, users preferred SUMMA ten times as often as flat MDS and three times as often as timelines.\n",
      "-The explosion in the number of documents on the Web necessitates automated approaches that organize and summarize large document collections on a complex topic.\n",
      "--Existing methods for multi-document summarization (MDS) are designed to produce short summaries of 10-15 documents.\n",
      "--MDS systems do not scale to data sets ten times larger and proportionately longer summaries: they either cannot run on large input or produce a disorganized summary that is difficult to understand.\n",
      "-We present a novel MDS paradigm, hierarchical summarization, which operates on large document collections, creating summaries that organize the information coherently.\n",
      "--It mimics how someone with a general interest in a complex topic would learn about it from an expert – first, the expert would provide an overview, and then more specific information about various aspects.\n",
      "---Hierarchical summarization has the following novel characteristics:\n",
      "----The summary is hierarchically organized along one or more organizational principles such as time, location, entities, or events.\n",
      "----Each non-leaf summary is associated with a set of child summaries where each gives details of an element (e.g. sentence) in the parent summary.\n",
      "----A user can navigate within the hierarchical summary by clicking on an element of a parent summary to view the associated child summary.\n",
      "--We describe SUMMA, the first hierarchical summarization system for multi-document summarization.\n",
      "---SUMMA operates on a corpus of related news articles.\n",
      "---SUMMA hierarchically clusters the sentences by time, and then summarizes the clusters using an objective function that optimizes salience and coherence.\n",
      "--We conducted an Amazon Mechanical Turk (AMT) evaluation where AMT workers compared the output of SUMMA to that of timelines and flat summaries.\n",
      "---SUMMA output was judged superior more than three times as often as timelines, and users learned more in twice as many cases.\n",
      "---Users overwhelmingly preferred hierarchical summaries to flat summaries (92%) and learned just as much.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = dict_to_node(extract_json_from_text(model_response.content))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446c6f53-248a-4eaf-96fb-084c83f04a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_tree = json_to_node('data/reference_maps/paper_0.json')\n",
    "scoring_model = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v1')\n",
    "def cos_dist(a_embedding, b_embedding):\n",
    "        return float(1 - scoring_model.similarity(a_embedding, b_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4d3f175-2b10-40a8-a5d7-66b5931ccdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.947549819946289\n"
     ]
    }
   ],
   "source": [
    "dist = text_tree_distance(tree, reference_tree, scoring_model.encode, cos_dist)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96153d1-f7a7-4c21-a105-ea1e984018e7",
   "metadata": {},
   "source": [
    "### Method 2: Automatic optimization of human prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc03bc-4d22-45e0-9283-28d941745485",
   "metadata": {},
   "source": [
    "First, let's create a wrapper for mind map generation and scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29501b9f-75a2-4786-acd4-19f1f9b73a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mind_map(prompt, document_text, model, reference_map, scorer):\n",
    "    '''\n",
    "    Generate mind map of document using given LLM and compare it with baseline map using scorer.\n",
    "\n",
    "    Arguments:\n",
    "        prompt: str - prompt template for mind map generation.\n",
    "        document_text: str - text of document to be summarized.\n",
    "        model - language model for mind map generation.\n",
    "        reference_map: Node - reference mind map for generation scoring.\n",
    "        scorer - comparison function for comparing the generated map and reference one.\n",
    "\n",
    "    Output:\n",
    "        conversation - dict containing user prompt and model response, presumably containing generated mind map.\n",
    "        feedback - dict containing a score for prompt optimization. Could also contain a comment in case of invalid map format.\n",
    "    '''\n",
    "    kTreeExample = '''{\n",
    "      \"A new algorithm for text tree edit distance based on Zhang-Shasha's algorithm and BERT-like model embedding similarity.\": {\n",
    "        \"The algorithm's novelty is in its similarity measure based on BERT-like model embeddings.\": {\n",
    "          \"Embedding distance is used as a measure of semantic similarity.\": {},\n",
    "          \"The language model allows to capture semantic meaning of sentences and model their similarity.\": {}\n",
    "        },\n",
    "        \"Zhang-Shasha's algorithm is used to compute tree edit distance with new edit costs.\": {\n",
    "          \"Semantic similarity is used as the update cost in the algorithm.\": {},\n",
    "          \"The costs of insertion and removal of nodes are defined as the similarity of the node and an empty sentence.\": {}\n",
    "        },\n",
    "        \"The proposed algorithm is presented as a more informative metric of similarity between text trees.\": {\n",
    "          \"The current ways of comparing text trees overlook overlook their tree structure or the meaning of their labels.\": {},\n",
    "          \"This new method can be used, for example, to compare mind maps or hierarchical summaries.\": {}\n",
    "        }\n",
    "      }\n",
    "    }'''\n",
    "    \n",
    "    full_prompt = prompt + document_text\n",
    "    model_response = prompt_model(full_prompt, model).content\n",
    "\n",
    "    conversation = [\n",
    "        {'role': 'user', 'content': full_prompt},\n",
    "        {'role': 'assistant', 'content': model_response},\n",
    "    ]\n",
    "\n",
    "    tree = dict_to_node(extract_json_from_text(model_response))\n",
    "    if tree == None:\n",
    "        feedback = {\n",
    "            'score': -100000,\n",
    "            'comment': 'Invalid format: response should contain one text tree in the JSON format as in following example: '+kTreeExample,\n",
    "        }\n",
    "    else:\n",
    "        score = scorer(tree, reference_map)\n",
    "        feedback = {'score': score}\n",
    "\n",
    "    return conversation, feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "377992e1-2715-454f-9604-1ea1af551429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:05<00:00, 16.36s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt_templates = [read_text('prompts/human_prompt_'+str(i)+'.txt') for i in range(4)]\n",
    "document_text = parse_pdf('data/papers/paper_0.pdf')\n",
    "trajectories = []\n",
    "\n",
    "model = ChatMistralAI(model='mistral-large-latest')\n",
    "\n",
    "reference_tree = json_to_node('data/reference_maps/paper_0.json')\n",
    "scoring_model = SentenceTransformer('sentence-transformers/paraphrase-distilroberta-base-v1')\n",
    "\n",
    "def cos_dist(a_embedding, b_embedding):\n",
    "        return float(1 - scoring_model.similarity(a_embedding, b_embedding))\n",
    "\n",
    "def scorer(tree_a, tree_b):\n",
    "    return -text_tree_distance(tree_a, tree_b, scoring_model.encode, cos_dist)\n",
    "\n",
    "for prompt in tqdm(prompt_templates):\n",
    "    conversation, feedback = generate_mind_map(prompt, document_text, model, reference_tree, scorer)\n",
    "\n",
    "    trajectories.append((conversation, feedback))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5927c294-fe56-4d0f-a928-01cd53148982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate a sentence-based mind map of the given scientific paper. Output it in the JSON format as in the following example:\\n{\\n  \"A new algorithm for text tree edit distance based on Zhang-Shasha\\'s algorithm and BERT-like model embedding similarity.\": {\\n    \"The algorithm\\'s novelty is in its similarity measure based on BERT-like model embeddings.\": {\\n      \"Embedding distance is used as a measure of semantic similarity.\": {},\\n      \"The language model allows to capture semantic meaning of sentences and model their similarity.\": {}\\n    },\\n    \"Zhang-Shasha\\'s algorithm is used to compute tree edit distance with new edit costs.\": {\\n      \"Semantic similarity is used as the update cost in the algorithm.\": {},\\n      \"The costs of insertion and removal of nodes are defined as the similarity of the node and an empty sentence.\": {}\\n    },\\n    \"The proposed algorithm is presented as a more informative metric of similarity between text trees.\": {\\n      \"The current ways of comparing text trees overlook overlook their tree structure or the meaning of their labels.\": {},\\n      \"This new method can be used, for example, to compare mind maps or hierarchical summaries.\": {}\\n    }\\n  }\\n}\\nOutput only the mind map in the format as shown above as plain text. The map should have no more than three hierarchy levels. Here is the text of the paper:\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = create_prompt_optimizer(\n",
    "    \"mistral-large-latest\",\n",
    "    kind=\"gradient\",\n",
    "    config={\"max_reflection_steps\": 5, \"min_reflection_steps\": 0},\n",
    ")\n",
    "\n",
    "updated = optimizer.invoke({\"trajectories\": trajectories, \"prompt\": prompt_templates[0]})\n",
    "updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ef7600-a25b-4cc1-96c7-01277d4539bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = updated + parse_pdf('data/papers/paper_0.pdf')\n",
    "model_response = prompt_model(prompt=prompt, model=model)\n",
    "tree = dict_to_node(extract_json_from_text(model_response.content))\n",
    "score = -text_tree_distance(tree, reference_tree, scoring_model.encode, cos_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c7c071e-eddd-4476-a2c8-06ea94e342d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Summarization: Scaling Up Multi-Document Summarization.\n",
      "-Multi-document summarization (MDS) systems have been designed for short, unstructured summaries of 10-15 documents, and are inadequate for larger document collections.\n",
      "--We propose a new approach to scaling up summarization called hierarchical summarization, and present the first implemented system, SUMMA.\n",
      "---SUMMA produces a hierarchy of relatively short summaries, in which the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest.\n",
      "---SUMMA optimizes for coherence as well as coverage of salient information.\n",
      "---In an Amazon Mechanical Turk evaluation, users preferred SUMMA ten times as often as flat MDS and three times as often as timelines.\n",
      "-The explosion in the number of documents on the Web necessitates automated approaches that organize and summarize large document collections on a complex topic.\n",
      "--Existing methods for multi-document summarization (MDS) are designed to produce short summaries of 10-15 documents.\n",
      "--MDS systems do not scale to data sets ten times larger and proportionately longer summaries: they either cannot run on large input or produce a disorganized summary that is difficult to understand.\n",
      "-We present a novel MDS paradigm, hierarchical summarization, which operates on large document collections, creating summaries that organize the information coherently.\n",
      "--It mimics how someone with a general interest in a complex topic would learn about it from an expert – first, the expert would provide an overview, and then more specific information about various aspects.\n",
      "--Hierarchical summarization has the following novel characteristics:\n",
      "---The summary is hierarchically organized along one or more organizational principles such as time, location, entities, or events.\n",
      "---Each non-leaf summary is associated with a set of child summaries where each gives details of an element (e.g. sentence) in the parent summary.\n",
      "---A user can navigate within the hierarchical summary by clicking on an element of a parent summary to view the associated child summary.\n",
      "\n",
      "-8.186757266521454\n"
     ]
    }
   ],
   "source": [
    "print(tree)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605eef50-d2ba-46f7-a7d3-8cd32921e0d0",
   "metadata": {},
   "source": [
    "Why does this method yield suboptimal results? The ProTeGi prompt optimization model (Pryzant et al., 2023) uses explicit feedback to generate textual gradients, while the only feedback we can provide automatically in our task is numerical scores or a pre-written message about invalid tree format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472d12c-59f0-404c-96b8-760e8ec665f7",
   "metadata": {},
   "source": [
    "### Method 3: Consecutive prompting and map construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b92d12d-6d91-4fe9-9d84-0df8bb335940",
   "metadata": {},
   "source": [
    "The idea behind this method is letting the user interact with the intelligent system and decide which details he would like to add to his map of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de6e6adf-6232-49a1-ad01-6f029b50f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt = f'''Study the following paper:\n",
    "{document_text}\n",
    "Write out the main idea of the paper in one sentence. Output only this sentence.'''\n",
    "chat_history = [HumanMessage(content=main_prompt)]\n",
    "main_idea = prompt_model(main_prompt, model).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec63431c-f109-4e40-864b-2bd8e3618716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper introduces a novel approach to large-scale multi-document summarization called hierarchical summarization, which organizes information in a coherent hierarchy and allows users to navigate and explore details based on their interests.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(main_idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62087e7e-759a-419a-9fba-e1f65445fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "details_prompt = '''Now, given the paper and its main idea, write 6 questions asking for details. \n",
    "Enumerate the questions and output only them.'''\n",
    "chat_history += [AIMessage(content=main_idea), HumanMessage(content=details_prompt)]\n",
    "questions = model.invoke(chat_history).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeefced9-afd7-418a-9a56-7fefc19f5805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What are the key characteristics of hierarchical summarization that make it different from traditional multi-document summarization?\n",
      "2. How does the SUMMA system implement hierarchical summarization, and what methodologies does it use?\n",
      "3. What are the metrics used to evaluate the quality of a hierarchical summary, and how are they defined?\n",
      "4. How does the SUMMA system ensure coherence between parent and child summaries in the hierarchy?\n",
      "5. What are the results of the user study comparing SUMMA with timelines and flat multi-document summaries?\n",
      "6. What are the future directions and improvements planned for the SUMMA system and hierarchical summarization?\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66ecc9d5-e7dc-493f-a188-e76798a0d3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input the numbers of questions you would like to get answers to:  1, 2, 5\n"
     ]
    }
   ],
   "source": [
    "selected_questions = input('Input the numbers of questions you would like to get answers to: ')\n",
    "question_answering_prompt = f'''Answer questions {selected_questions} according to the paper. \n",
    "Each answer should be one sentence long. Output only the answers without enumeration, each from a new line.'''\n",
    "chat_history += [AIMessage(content=questions), HumanMessage(content=question_answering_prompt)]\n",
    "answers = model.invoke(chat_history).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff567ba7-07de-4c1e-bd0f-22f5cff0a86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical summarization organizes information along principles such as time, location, entities, or events, allowing users to navigate from a general overview to more detailed child summaries.\n",
      "The SUMMA system implements hierarchical summarization by first clustering sentences temporally and then summarizing these clusters using an objective function that optimizes for salience and coherence.\n",
      "The user study found that participants preferred SUMMA over timelines three times as often and over flat summaries ten times as often, and they learned just as much from SUMMA as from flat summaries.\n"
     ]
    }
   ],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd2210fb-c7d2-4a7f-8c0a-d6a56fb99b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_details_prompt = '''Now, given the paper and these points, write 6 questions for each point asking for details. \n",
    "Enumerate the questions and output only them.'''\n",
    "chat_history += [AIMessage(content=answers), HumanMessage(content=more_details_prompt)]\n",
    "more_questions = model.invoke(chat_history).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40698a8f-1f72-4c27-a47d-ed9d4d417f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Point 1: Hierarchical Summarization Characteristics\n",
      "\n",
      "1. How does hierarchical summarization ensure that the information presented at the start is small and grows only as the user directs it?\n",
      "2. What specific mechanisms does hierarchical summarization use to allow users to tailor the output to their interests?\n",
      "3. How does the organization of summaries along principles like time, location, entities, or events contribute to the coherence of the information?\n",
      "4. What are the advantages of hierarchical summarization over traditional flat summaries in terms of user navigation and exploration?\n",
      "5. How does the hierarchical structure help in managing large document collections without overwhelming the user?\n",
      "6. What are the different organizing principles that can be used in hierarchical summarization, and how are they selected for different portions of the hierarchy?\n",
      "\n",
      "### Point 2: SUMMA System Implementation\n",
      "\n",
      "1. What is the role of temporal clustering in the SUMMA system, and how is it implemented?\n",
      "2. How does the SUMMA system ensure that the summaries are coherent and salient?\n",
      "3. What specific methodologies does the SUMMA system use to optimize for salience and coherence in the summaries?\n",
      "4. How does the SUMMA system handle the complexity of inducing a hierarchical structure and enforcing coherence between parent and child summaries?\n",
      "5. What is the process of hierarchical clustering in the SUMMA system, and how does it determine the structure of the hierarchical summary?\n",
      "6. How does the SUMMA system summarize given the hierarchy, and what role does the hierarchical structure play in this process?\n",
      "\n",
      "### Point 3: User Study Results\n",
      "\n",
      "1. What were the criteria used to evaluate user preference in the study comparing SUMMA with timelines and flat summaries?\n",
      "2. How did the user study measure the effectiveness of SUMMA in helping users learn about complex topics compared to timelines and flat summaries?\n",
      "3. What specific aspects of SUMMA did users find more preferable compared to timelines and flat summaries?\n",
      "4. How did the user study ensure that the results were not biased, and what methods were used to control for spam and low-quality responses?\n",
      "5. What were the reasons given by users for preferring SUMMA over timelines and flat summaries?\n",
      "6. How did the user study compare the informativeness and coherence of SUMMA summaries with those of timelines and flat summaries?\n"
     ]
    }
   ],
   "source": [
    "print(more_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b248b6af-c2dc-4ed5-bae2-e78c9351badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input the numbers of questions you would like to get answers to:  1.4, 2.3, 2.5, 2.6, 3.1, 3.2, 3.6\n"
     ]
    }
   ],
   "source": [
    "selected_questions = input('Input the numbers of questions you would like to get answers to: ')\n",
    "question_answering_prompt = f'''Answer questions {selected_questions} according to the paper. \n",
    "Each answer should be one sentence long. Output only the answers without enumeration, each from a new line.'''\n",
    "chat_history += [AIMessage(content=more_questions), HumanMessage(content=question_answering_prompt)]\n",
    "answers = model.invoke(chat_history).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4317a635-4bfc-4299-806e-b61f76da311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical summarization allows users to start with a general overview and drill down into more detailed information on topics of interest, making it easier to manage and understand large document collections.\n",
      "The SUMMA system uses an objective function that balances salience and coherence, treating redundancy and budget as hard constraints, to generate high-quality summaries.\n",
      "The process of hierarchical clustering in the SUMMA system involves recursively clustering sentences based on temporal information, using a method that automatically chooses the appropriate number of clusters at each split.\n",
      "The SUMMA system summarizes within the hierarchy by following the hierarchical structure of the clustering, where each node has an associated flat summary, and the number of sentences in a flat summary is equal to the number of child clusters of the node.\n",
      "The user study evaluated user preference by asking participants to choose which format they preferred (SUMMA, timelines, or flat summaries) and to explain why.\n",
      "The user study measured the effectiveness of SUMMA in helping users learn about complex topics by asking participants to write a paragraph summarizing the information they learned after reading the summaries.\n",
      "The user study compared the informativeness and coherence of SUMMA summaries with those of timelines and flat summaries by using ROUGE scores for coverage and manual evaluation for recall of important events.\n"
     ]
    }
   ],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72e1913a-399d-4317-acc0-cabd3ef66e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_map = dict_to_node({\n",
    "    'The paper introduces a novel approach to large-scale multi-document summarization called hierarchical summarization, which organizes information in a coherent hierarchy and allows users to navigate and explore details based on their interests.': {\n",
    "        'Hierarchical summarization organizes information along principles such as time, location, entities, or events, allowing users to navigate from a general overview to more detailed child summaries.': {\n",
    "            'Hierarchical summarization allows users to start with a general overview and drill down into more detailed information on topics of interest, making it easier to manage and understand large document collections.': {}\n",
    "        },\n",
    "        'The SUMMA system implements hierarchical summarization by first clustering sentences temporally and then summarizing these clusters using an objective function that optimizes for salience and coherence.': {\n",
    "            'The SUMMA system uses an objective function that balances salience and coherence, treating redundancy and budget as hard constraints, to generate high-quality summaries.': {},\n",
    "            'The process of hierarchical clustering in the SUMMA system involves recursively clustering sentences based on temporal information, using a method that automatically chooses the appropriate number of clusters at each split.': {},\n",
    "            'The SUMMA system summarizes within the hierarchy by following the hierarchical structure of the clustering, where each node has an associated flat summary, and the number of sentences in a flat summary is equal to the number of child clusters of the node.': {}\n",
    "        },\n",
    "        'The user study found that participants preferred SUMMA over timelines three times as often and over flat summaries ten times as often, and they learned just as much from SUMMA as from flat summaries.': {\n",
    "            'The user study evaluated user preference by asking participants to choose which format they preferred (SUMMA, timelines, or flat summaries) and to explain why.': {},\n",
    "            'The user study measured the effectiveness of SUMMA in helping users learn about complex topics by asking participants to write a paragraph summarizing the information they learned after reading the summaries.': {},\n",
    "            'The user study compared the informativeness and coherence of SUMMA summaries with those of timelines and flat summaries by using ROUGE scores for coverage and manual evaluation for recall of important events.': {}\n",
    "        }\n",
    "    }\n",
    "})\n",
    "score = -text_tree_distance(generated_map, reference_tree, scoring_model.encode, cos_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82182525-43e9-48d6-9108-9c7ad4b0cd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.9009242057800293\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ddc7b6-bcd8-41ec-8487-424651072f30",
   "metadata": {},
   "source": [
    "This approach looks more promising in terms of generating a map closer to what the user would have made by himself. Let's try to automate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61a2622f-3e2c-47ee-9862-0dad02905a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First prompt to make the LLM familiar with the text, explain the task and generate the main idea (root node of the summary).\n",
    "def start_prompt(document_text: str):\n",
    "    return f'''Carefully study the following research paper:\n",
    "    {document_text}\n",
    "    This is the end of the paper. Now we are going to make a hierarchical summary of the paper.\n",
    "    The purpose of this summary is to systematize the research study, going from key points to more specific details.\n",
    "    The result should be a hierarchical list of one-sentence points based on the paper. They shouldn't necessarily be sentences from the text.\n",
    "    Organize the summary by enumerating it like in the following example:\n",
    "    This is the main idea.\n",
    "    1. This is key point number 1.\n",
    "    1.1. This is a detail for point number 1.\n",
    "    1.2. This is another detail for point number 1.\n",
    "    2. This is another key point.\n",
    "    2.1 This is a detail for point number 2.\n",
    "    We are going to generate the summary iteratively the following way: \n",
    "    - You write out the main idea of the text and generate questions aimed to provide details to the main idea.\n",
    "    - The user picks the questions he wants to be answered.\n",
    "    - You answer the questions, each in one sentence, and add them to the summary under the main idea as the first level of hierarchy in the summary.\n",
    "    - The user picks a point he wants to get details on.\n",
    "    - You generate questions aimed to provide details for this point.\n",
    "    - The user picks the questions that interest them.\n",
    "    - You answer each question in one sentence and add these sentences to the summary under the point which you generated questions for.\n",
    "    - The process repeats until the user tells you that the summary is complete.\n",
    "    Let's start now. Write out the main idea of the paper in one sentence. Output only this sentence.'''\n",
    "\n",
    "\n",
    "# Prompt to generate questions for the second layer of the summary.\n",
    "main_idea_questions_prompt = '''Now, generate 6 questions aimed to add details to the main idea.\n",
    "    Enumerate the questions and output only them.'''\n",
    "\n",
    "\n",
    "# Prompt to generate questions for point with given number.\n",
    "def point_questions_prompt(point_number):\n",
    "    return f'''Generate 6 questions aimed to add details to point number {point_number}.\n",
    "    Enumerate the questions and output only them.'''\n",
    "\n",
    "\n",
    "# Prompt to answer questions with given numbers.\n",
    "def answer_questions_prompt(selected_questions):\n",
    "    return f'''Now answer questions {selected_questions} and add the answers to them to the summary.\n",
    "    Add them under the point for which you generated the questions previously.\n",
    "    The answers to each question should be only one sentence long and add only one point to our summary. \n",
    "    Make sure ALL the point we are adding to summary right now are on ONE level of hierarchy.\n",
    "    Output the whole summary we have thus far made and only it. Make sure the enumeration in the summary is as in the example I provided.'''\n",
    "\n",
    "\n",
    "# Last prompt to convert the mind map to the format we are working with.\n",
    "convert_map_prompt = '''We have finished building our summary. Now your job is to convert it to a JSON text tree.\n",
    "Output it in the JSON format as in the following example:\n",
    "{\n",
    "  \"A new algorithm for text tree edit distance based on Zhang-Shasha's algorithm and BERT-like model embedding similarity.\": {\n",
    "    \"The algorithm's novelty is in its similarity measure based on BERT-like model embeddings.\": {\n",
    "      \"Embedding distance is used as a measure of semantic similarity.\": {},\n",
    "      \"The language model allows to capture semantic meaning of sentences and model their similarity.\": {}\n",
    "    },\n",
    "    \"Zhang-Shasha's algorithm is used to compute tree edit distance with new edit costs.\": {\n",
    "      \"Semantic similarity is used as the update cost in the algorithm.\": {},\n",
    "      \"The costs of insertion and removal of nodes are defined as the similarity of the node and an empty sentence.\": {}\n",
    "    },\n",
    "    \"The proposed algorithm is presented as a more informative metric of similarity between text trees.\": {\n",
    "      \"The current ways of comparing text trees overlook overlook their tree structure or the meaning of their labels.\": {},\n",
    "      \"This new method can be used, for example, to compare mind maps or hierarchical summaries.\": {}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "Output only the mind map we made in this format.'''\n",
    "\n",
    "\n",
    "# This prompt is for cases when the model outputs the mind map in the wrong format\n",
    "wrong_format_prompt = '''It seems that you have output the map in the wrong format. Take a look at the example again:\n",
    "{\n",
    "  \"A new algorithm for text tree edit distance based on Zhang-Shasha's algorithm and BERT-like model embedding similarity.\": {\n",
    "    \"The algorithm's novelty is in its similarity measure based on BERT-like model embeddings.\": {\n",
    "      \"Embedding distance is used as a measure of semantic similarity.\": {},\n",
    "      \"The language model allows to capture semantic meaning of sentences and model their similarity.\": {}\n",
    "    },\n",
    "    \"Zhang-Shasha's algorithm is used to computqe tree edit distance with new edit costs.\": {\n",
    "      \"Semantic similarity is used as the update cost in the algorithm.\": {},\n",
    "      \"The costs of insertion and removal of nodes are defined as the similarity of the node and an empty sentence.\": {}\n",
    "    },\n",
    "    \"The proposed algorithm is presented as a more informative metric of similarity between text trees.\": {\n",
    "      \"The current ways of comparing text trees overlook overlook their tree structure or the meaning of their labels.\": {},\n",
    "      \"This new method can be used, for example, to compare mind maps or hierarchical summaries.\": {}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "This is the map in the JSON format we should get from the following hierarchical summary:\n",
    "A new algorithm for text tree edit distance based on Zhang-Shasha's algorithm and BERT-like model embedding similarity.\n",
    "1. The algorithm's novelty is in its similarity measure based on BERT-like model embeddings.\n",
    "1.1. Embedding distance is used as a measure of semantic similarity.\n",
    "1.2. The language model allows to capture semantic meaning of sentences and model their similarity.\n",
    "2. Zhang-Shasha's algorithm is used to compute tree edit distance with new edit costs.\n",
    "2.1. Semantic similarity is used as the update cost in the algorithm.\n",
    "2.2. The costs of insertion and removal of nodes are defined as the similarity of the node and an empty sentence.\n",
    "3. The proposed algorithm is presented as a more informative metric of similarity between text trees.\n",
    "3.1. The current ways of comparing text trees overlook overlook their tree structure or the meaning of their labels.\n",
    "3.2. This new method can be used, for example, to compare mind maps or hierarchical summaries.\n",
    "\n",
    "Now, given the example, output the JSON map in the correct format.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "010f333b-266d-4b5c-86ac-6b1c347ee32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_iteratively(document_text, model):\n",
    "    '''\n",
    "    Generate hierarchical summary of given text by iteratively prompting given LLM. \n",
    "    '''\n",
    "    chat = [HumanMessage(content=start_prompt(document_text))]\n",
    "\n",
    "    main_idea = model.invoke(chat).content\n",
    "    print(main_idea)\n",
    "    \n",
    "    chat += [\n",
    "        AIMessage(content=main_idea),\n",
    "        HumanMessage(content=main_idea_questions_prompt)\n",
    "    ]\n",
    "\n",
    "    main_idea_questions = model.invoke(chat).content\n",
    "    chat.append(AIMessage(content=main_idea_questions))\n",
    "    print(main_idea_questions)\n",
    "\n",
    "    while(True):\n",
    "        selected_questions = input('Select questions to answer: ')\n",
    "        if selected_questions == 'q':\n",
    "            break\n",
    "        chat.append(HumanMessage(content=answer_questions_prompt(selected_questions)))\n",
    "\n",
    "        current_map = model.invoke(chat).content\n",
    "        chat.append(AIMessage(content=current_map))\n",
    "        print(current_map)\n",
    "\n",
    "        selected_point = input('Select point to generate questions for: ')\n",
    "        if selected_point == 'q':\n",
    "            break\n",
    "        chat.append(HumanMessage(content=point_questions_prompt(selected_point)))\n",
    "        \n",
    "        new_questions = model.invoke(chat).content\n",
    "        chat.append(AIMessage(content=new_questions))\n",
    "        print(new_questions)\n",
    "\n",
    "    chat.append(HumanMessage(content=convert_map_prompt))\n",
    "\n",
    "    conversion_successful = False\n",
    "    while not conversion_successful:\n",
    "        converted_map = model.invoke(chat).content\n",
    "        chat.append(AIMessage(content=converted_map))\n",
    "        \n",
    "        json = extract_json_from_text(converted_map)\n",
    "        if json is not None:\n",
    "            try:\n",
    "                tree = dict_to_node(json)\n",
    "                conversion_successful = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if not conversion_successful:\n",
    "            print('Conversion unsuccessful, trying again...')\n",
    "            chat.append(HumanMessage(content=wrong_format_prompt))\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "60d9d1a4-9c84-4c5b-a888-237690522cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper introduces hierarchical summarization, a novel approach to multi-document summarization that organizes large document collections coherently.\n",
      "1. What is the main problem with existing multi-document summarization systems that hierarchical summarization aims to solve?\n",
      "2. What are the key characteristics of hierarchical summarization?\n",
      "3. How does the SUMMA system, the first implementation of hierarchical summarization, operate?\n",
      "4. What are the main evaluation criteria used to assess the effectiveness of hierarchical summarization in the user study?\n",
      "5. How does hierarchical summarization compare to traditional multi-document summarization and timelines in terms of user preference and knowledge acquisition?\n",
      "6. What are the future directions for improving and expanding the hierarchical summarization approach?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select questions to answer:  2, 3, 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper introduces hierarchical summarization, a novel approach to multi-document summarization that organizes large document collections coherently.\n",
      "1. Hierarchical summarization organizes summaries along principles such as time, location, entities, or events, with each non-leaf summary linked to child summaries for more details.\n",
      "2. The SUMMA system, the first implementation of hierarchical summarization, operates by hierarchically clustering sentences by time and summarizing these clusters using an objective function that optimizes salience and coherence.\n",
      "3. In a user study, hierarchical summarization was preferred over traditional multi-document summarization and timelines, with users learning just as much or more from hierarchical summaries.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select point to generate questions for:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the first step in the SUMMA system's process for hierarchical summarization?\n",
      "2. How does the SUMMA system determine the hierarchical structure of the summaries?\n",
      "3. What objective function does the SUMMA system use to optimize salience and coherence?\n",
      "4. What method does the SUMMA system use to ensure temporal coherence in the summaries?\n",
      "5. How does the SUMMA system handle the selection of the number of clusters at each level of the hierarchy?\n",
      "6. What algorithm does the SUMMA system use to approximate the optimization of the objective function?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select questions to answer:  2, 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper introduces hierarchical summarization, a novel approach to multi-document summarization that organizes large document collections coherently.\n",
      "1. Hierarchical summarization organizes summaries along principles such as time, location, entities, or events, with each non-leaf summary linked to child summaries for more details.\n",
      "2. The SUMMA system, the first implementation of hierarchical summarization, operates by hierarchically clustering sentences by time and summarizing these clusters using an objective function that optimizes salience and coherence.\n",
      "2.1. The SUMMA system uses hierarchical clustering to organize sentences into manageable, semantically-related sections, inducing a hierarchical structure over the input.\n",
      "2.2. The SUMMA system's objective function balances salience and coherence, treating redundancy and budget as hard constraints while optimizing for coherence and salience as soft constraints.\n",
      "3. In a user study, hierarchical summarization was preferred over traditional multi-document summarization and timelines, with users learning just as much or more from hierarchical summaries.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select point to generate questions for:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What specific metrics were used to evaluate the effectiveness of hierarchical summarization in the user study?\n",
      "2. How did the user study compare hierarchical summarization to traditional multi-document summarization and timelines?\n",
      "3. What were the key findings regarding user preference between hierarchical summarization and traditional methods?\n",
      "4. How was knowledge acquisition assessed in the user study for hierarchical summarization compared to other methods?\n",
      "5. What were the results of the ROUGE evaluation for hierarchical summarization compared to other methods?\n",
      "6. How did the manual evaluation of event recall compare between hierarchical summarization and other methods?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select questions to answer:  1, 3, 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper introduces hierarchical summarization, a novel approach to multi-document summarization that organizes large document collections coherently.\n",
      "1. Hierarchical summarization organizes summaries along principles such as time, location, entities, or events, with each non-leaf summary linked to child summaries for more details.\n",
      "2. The SUMMA system, the first implementation of hierarchical summarization, operates by hierarchically clustering sentences by time and summarizing these clusters using an objective function that optimizes salience and coherence.\n",
      "2.1. The SUMMA system uses hierarchical clustering to organize sentences into manageable, semantically-related sections, inducing a hierarchical structure over the input.\n",
      "2.2. The SUMMA system's objective function balances salience and coherence, treating redundancy and budget as hard constraints while optimizing for coherence and salience as soft constraints.\n",
      "3. In a user study, hierarchical summarization was preferred over traditional multi-document summarization and timelines, with users learning just as much or more from hierarchical summaries.\n",
      "3.1. The user study evaluated user preference, knowledge acquisition, informativeness, and coherence to assess the effectiveness of hierarchical summarization.\n",
      "3.2. Users preferred hierarchical summarization over traditional multi-document summarization and timelines, with a significant preference for hierarchical summaries.\n",
      "3.3. Knowledge acquisition was assessed by having users write a paragraph summarizing what they learned, with descriptions from hierarchical summarization preferred over timelines and equally informative as flat MDS.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select point to generate questions for:  q\n"
     ]
    }
   ],
   "source": [
    "generated_map = generate_summary_iteratively(document_text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42307922-3dd8-4148-9468-50810de1e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper introduces hierarchical summarization, a novel approach to multi-document summarization that organizes large document collections coherently.\n",
      "-Hierarchical summarization organizes summaries along principles such as time, location, entities, or events, with each non-leaf summary linked to child summaries for more details.\n",
      "-The SUMMA system, the first implementation of hierarchical summarization, operates by hierarchically clustering sentences by time and summarizing these clusters using an objective function that optimizes salience and coherence.\n",
      "--The SUMMA system uses hierarchical clustering to organize sentences into manageable, semantically-related sections, inducing a hierarchical structure over the input.\n",
      "--The SUMMA system's objective function balances salience and coherence, treating redundancy and budget as hard constraints while optimizing for coherence and salience as soft constraints.\n",
      "-In a user study, hierarchical summarization was preferred over traditional multi-document summarization and timelines, with users learning just as much or more from hierarchical summaries.\n",
      "--The user study evaluated user preference, knowledge acquisition, informativeness, and coherence to assess the effectiveness of hierarchical summarization.\n",
      "--Users preferred hierarchical summarization over traditional multi-document summarization and timelines, with a significant preference for hierarchical summaries.\n",
      "--Knowledge acquisition was assessed by having users write a paragraph summarizing what they learned, with descriptions from hierarchical summarization preferred over timelines and equally informative as flat MDS.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generated_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e2445e07-c032-43d9-88a3-1aeae7a48f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.3494465947151184\n"
     ]
    }
   ],
   "source": [
    "score = -text_tree_distance(generated_map, reference_tree, scoring_model.encode, cos_dist)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
